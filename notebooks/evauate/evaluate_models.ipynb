{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoAdapterModel,\n",
    "    TrainingArguments, \n",
    "    AdapterTrainer,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    DatasetDict\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import torch\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../data/processed\"\n",
    "MODEL_PATH = \"../../models\"\n",
    "\n",
    "dimensions = [\n",
    "    \"gender\", \n",
    "    \"race\", \n",
    "    \"religion\", \n",
    "    \"nationality\", \n",
    "    \"country\",\n",
    "    \"merged\"\n",
    "]\n",
    "\n",
    "tasks = [\n",
    "    \"buzzfeed\",\n",
    "    \"politifact\",\n",
    "    \"twittercovidq2\",\n",
    "    \"clef22\",\n",
    "    \"propaganda\",\n",
    "    \"webis\",\n",
    "    \"pheme\",\n",
    "    \"basil\",\n",
    "    \"shadesoftruth\",\n",
    "    \"fingerprints\",\n",
    "    \"clickbait\",\n",
    "]\n",
    "\n",
    "testset_state = [\n",
    "    \"unperturbed\",\n",
    "    \"perturbed\"\n",
    "]\n",
    "\n",
    "models_name = [\n",
    "    \"bert-base-cased\",\n",
    "    \"roberta-base\",\n",
    "    \"distilbert-base-cased\",\n",
    "    \"microsoft/deberta-base\",\n",
    "    \"facebook/FairBERTa\",\n",
    "]\n",
    "\n",
    "model_finetunes = [\n",
    "    \"unperturbed\",\n",
    "    \"perturbed\"    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dimension in dimensions:\n",
    "    for task in tasks:\n",
    "        for state in testset_state:\n",
    "            for model_name in models_name:\n",
    "                for model_finetune in model_finetunes:\n",
    "                    # DIMENSION\n",
    "                    if model_finetune == \"unperturbed\":\n",
    "                        model_folder_path = f\"../..{os.sep}models{os.sep}{model_name}{os.sep}vanilla{os.sep}{task}\"\n",
    "                    else:\n",
    "                        model_folder_path = f\"../..{os.sep}models{os.sep}{model_name}{os.sep}{dimension}{os.sep}{task}\"\n",
    "                    \n",
    "                    CONFIG = {\n",
    "                        \"task_name\": task,\n",
    "                        \"model_name\": model_name,\n",
    "                        \"model_path\": f\"{model_folder_path}{os.sep}{os.listdir(model_folder_path)[0]}{os.sep}{task}\",\n",
    "                        \"max_length\": 128,\n",
    "                    }\n",
    "\n",
    "                    dataset_path = f\"{DATA_PATH}{os.sep}{dimension}\"\n",
    "\n",
    "                    test_df = pd.read_csv(f\"{dataset_path}{os.sep}{CONFIG['task_name']}_test.csv\")\n",
    "                    \n",
    "                    if state == \"perturbed\":\n",
    "                        test_df = test_df.drop(columns=['text'])\n",
    "                        test_df.rename(columns = {'perturbed_text':'text'}, inplace = True)\n",
    "\n",
    "                    test = Dataset.from_pandas(test_df)\n",
    "\n",
    "                    dataset = DatasetDict({\"test\": test})\n",
    "                    dataset = dataset.class_encode_column(\"labels\")\n",
    "\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "\n",
    "                    def tokenize_function(examples):\n",
    "                        return tokenizer(\n",
    "                            examples[\"text\"], padding=\"max_length\", truncation=True, max_length=CONFIG[\"max_length\"]\n",
    "                        )\n",
    "\n",
    "                    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "                    test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "                    model = AutoAdapterModel.from_pretrained(CONFIG['model_name'])\n",
    "                    model.load_adapter(CONFIG['model_path'])\n",
    "                    model.set_active_adapters(task)\n",
    "\n",
    "                    f1_metric = evaluate.load(\"f1\")\n",
    "                    recall_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "                    def compute_metrics(eval_pred):\n",
    "                        logits, labels = eval_pred\n",
    "                        preds = np.argmax(logits, axis=-1)\n",
    "                        results = {}\n",
    "                        results.update(f1_metric.compute(predictions=preds, references=labels, average=\"macro\"))\n",
    "                        results.update(recall_metric.compute(predictions=preds, references=labels))\n",
    "                        return results\n",
    "\n",
    "                    training_args = TrainingArguments(\n",
    "                        output_dir=\"evaluation\",\n",
    "                    )\n",
    "\n",
    "                    trainer = AdapterTrainer(\n",
    "                        model=model,\n",
    "                        args=training_args,\n",
    "                        compute_metrics=compute_metrics,\n",
    "                    )\n",
    "\n",
    "                    trainer.evaluate(test_dataset, metric_key_prefix=\"test\")\n",
    "\n",
    "                    del model\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "misinfo_bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
