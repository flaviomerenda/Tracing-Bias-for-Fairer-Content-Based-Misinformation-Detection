{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from adapters import (\n",
    "    AutoAdapterModel,\n",
    "    AdapterTrainer,\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    DatasetDict\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../data/processed\"\n",
    "MODEL_PATH = \"../../models\"\n",
    "\n",
    "testset_state = [\n",
    "    \"unperturbed\",\n",
    "    \"perturbed\"\n",
    "]\n",
    "\n",
    "dimensions = [\n",
    "    \"gender\", \n",
    "    \"race\", \n",
    "    \"religion\", \n",
    "    \"nationality\", \n",
    "    \"country\",\n",
    "    \"merged\"\n",
    "]\n",
    "\n",
    "models_name = [\n",
    "    \"roberta-base\",\n",
    "    \"facebook/FairBERTa\",\n",
    "    \"microsoft/deberta-base\",\n",
    "    \"bert-base-cased\",\n",
    "    \"distilbert-base-cased\"\n",
    "]\n",
    "\n",
    "tasks = [\n",
    "    \"clef22\",\n",
    "    \"fingerprints\",\n",
    "    \"shadesoftruth\",\n",
    "    \"basil\",\n",
    "    \"webis\",\n",
    "    \"pheme\",\n",
    "    \"clickbait\",\n",
    "    \"propaganda\",\n",
    "    \"politifact\",\n",
    "    \"buzzfeed\",\n",
    "    \"twittercovidq2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_ent_preds = False\n",
    "\n",
    "for model_name in models_name:\n",
    "    for dimension in dimensions:\n",
    "        for task in tasks:\n",
    "            for state in testset_state:\n",
    "\n",
    "                # DIMENSION\n",
    "                model_folder_path = f\"../..{os.sep}models{os.sep}{model_name}{os.sep}{dimension}{os.sep}{task}\"\n",
    "                CONFIG = {\n",
    "                    \"task_name\": task,\n",
    "                    \"model_name\": model_name,\n",
    "                    \"model_path\": f\"{model_folder_path}{os.sep}{os.listdir(model_folder_path)[0]}{os.sep}{task}\",\n",
    "                    \"max_length\": 128,\n",
    "                }\n",
    "\n",
    "                dataset_path = f\"{DATA_PATH}{os.sep}{dimension}\"\n",
    "\n",
    "                test_df = pd.read_csv(f\"{dataset_path}{os.sep}{CONFIG['task_name']}_test.csv\")\n",
    "                \n",
    "                if only_ent_preds:\n",
    "                    test_df = test_df.dropna()\n",
    "                if state == \"perturbed\":\n",
    "                    test_df = test_df.drop(columns=['text'])\n",
    "                    test_df.rename(columns = {'perturbed_text':'text'}, inplace = True)\n",
    "\n",
    "                test = Dataset.from_pandas(test_df)\n",
    "\n",
    "                dataset = DatasetDict({\"test\": test})\n",
    "                dataset = dataset.class_encode_column(\"labels\")\n",
    "\n",
    "                tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "\n",
    "                def tokenize_function(examples):\n",
    "                    return tokenizer(\n",
    "                        examples[\"text\"], padding=\"max_length\", truncation=True, max_length=CONFIG[\"max_length\"]\n",
    "                    )\n",
    "\n",
    "                tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "                test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "                model = AutoAdapterModel.from_pretrained(CONFIG['model_name'])\n",
    "                model.load_adapter(CONFIG['model_path'])\n",
    "                model.set_active_adapters(task)\n",
    "                \n",
    "                trainer = AdapterTrainer(model=model)\n",
    "\n",
    "                trainer.model = model.cuda()\n",
    "                y = trainer.predict(test_dataset)\n",
    "                preds = y.predictions[0] if isinstance(y.predictions, tuple) else y.predictions\n",
    "                if only_ent_preds:\n",
    "                    filename = f'../../data/interim/predictions{os.sep}{model_name}{os.sep}{dimension}{os.sep}{task}{os.sep}{state}.npy'\n",
    "                else:\n",
    "                    filename = f'../../data/interim/tot_predictions{os.sep}{model_name}{os.sep}{dimension}{os.sep}{task}{os.sep}{state}.npy'\n",
    "                if preds is not None:\n",
    "                    preds = np.argmax(preds, axis=1)\n",
    "                    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "                    with open(filename, 'wb') as f:\n",
    "                        np.save(f, preds)\n",
    "                else:\n",
    "                    print(f\"No examples for: {filename}\")\n",
    "\n",
    "                del model\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "misinfo_bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
