{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/perturber\").to(device=\"cuda\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/perturber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION = \"gender\"\n",
    "ENTITY_TYPE = [\"PERSON\", \"ORG\"]\n",
    "TARGET = \"woman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = \"../../data/raw\"\n",
    "PROCESSED_PATH = \"../../data/processed\"\n",
    "SEP = \"<PERT_SEP>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_entities = pd.read_csv(\"../../heterogeneity/lists_for_perturbations/names_swaps.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_to_entity =  pd.read_csv(\"../../heterogeneity/lists_for_perturbations/twitter_user_to_entity.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_to_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_entity(text, entity, df, search_column, return_column):\n",
    "    # Check if the substring is present in the search column\n",
    "    mask = df[search_column].str.contains(entity)\n",
    "\n",
    "    # If a match is found, return the corresponding value from the return column\n",
    "    if mask.any():\n",
    "        swap_entity = df.loc[mask, return_column].iloc[0]\n",
    "        regex = f\"([A-Z]([a-z]+|\\.)\\s*)*{entity.split(' ')[-1]}\"\n",
    "        text = re.sub(r''+regex, swap_entity, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text_entity_to_substitute = (text, None)\n",
    "    tw_users = twitter_to_entity[\"User\"].to_list()\n",
    "    for entity in tw_users:\n",
    "        if entity in text:\n",
    "            text = replace_entity(text, entity, twitter_to_entity, \"User\", \"UserSwap\")\n",
    "    # Create Doc object\n",
    "    doc2 = nlp(text)\n",
    "    # Identify the entities\n",
    "    entities = [ent.text for ent in doc2.ents if ent.label_ in ENTITY_TYPE]\n",
    "    if entities:\n",
    "        matched_entities = []\n",
    "        for entity in entities:\n",
    "            if any(entity in token for token in target_entities['NamedEntity'].to_list()):\n",
    "                is_substring_present = target_entities['NamedEntity'].str.contains(entity)\n",
    "                entity_to_substitute = target_entities.loc[is_substring_present, 'NamedEntity'].values[0]\n",
    "                if entity != entity_to_substitute:\n",
    "                    text = text.replace(entity, entity_to_substitute)\n",
    "                matched_entities.append((text, entity_to_substitute))\n",
    "        if matched_entities:\n",
    "            text_entity_to_substitute = matched_entities[0]\n",
    "\n",
    "    return text_entity_to_substitute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = []\n",
    "for path in os.walk(RAW_PATH):\n",
    "    for file in path[2]:\n",
    "        if file.endswith(\"test.csv\") or file.endswith(\"train.csv\"):\n",
    "            data_paths.append(f\"{path[0]}{os.sep}{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(sents, batch_size):\n",
    "    for i in range(0, len(sents), batch_size):\n",
    "        yield sents[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in data_paths:\n",
    "\n",
    "    path2check = f'{PROCESSED_PATH}{os.sep}{DIMENSION}{os.sep}{path.split(\"/\")[-1]}'\n",
    "    if os.path.exists(path2check):\n",
    "        print(f\"Already exists file: {path2check}. Skip.\")\n",
    "        continue\n",
    "\n",
    "    data = pd.read_csv(path).dropna()\n",
    "\n",
    "    print(f\"Processing {path}\")\n",
    "\n",
    "    print(\"Truncating texts...\")\n",
    "    data['text'] = data.apply(\n",
    "    lambda row: tokenizer.batch_decode(\n",
    "        tokenizer(\n",
    "            row.text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "        )[\"input_ids\"],\n",
    "    skip_special_tokens=True\n",
    "    )[0],\n",
    "    axis = 1\n",
    "    )\n",
    "    print(\"...texts truncated!\")\n",
    "\n",
    "    print(\"preprocessing text and extracting entities...\")\n",
    "    preprocessed_text = []\n",
    "    entities = []\n",
    "    for row in data[\"text\"].to_list():\n",
    "        text, entity = preprocess_text(row)\n",
    "        preprocessed_text.append(text)\n",
    "        entities.append(entity)\n",
    "    data[\"preprocessed_text\"] = preprocessed_text\n",
    "    data[\"entity\"] = entities\n",
    "    print(\"...text preprocessed and entities extracted!\")\n",
    "\n",
    "    data['perturber_input'] = data.apply(\n",
    "        lambda row: f\"{row.entity.split(' ')[0]}, {TARGET} {SEP} {row.preprocessed_text}\" \n",
    "        if row.entity else row.preprocessed_text, axis=1\n",
    "    )\n",
    "\n",
    "    print(\"Perturbating texts...\")\n",
    "    perturber_input = data[\"perturber_input\"].to_list()\n",
    "    entities = data[\"entity\"].to_list()\n",
    "    with torch.no_grad():\n",
    "        perturbed_texts = []\n",
    "        for input, entity in zip(tqdm(perturber_input, total=len(perturber_input)), entities):\n",
    "            if entity:\n",
    "                tokenized_batch = tokenizer(\n",
    "                    input,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                    max_length=128\n",
    "                )\n",
    "                outputs = model.generate(\n",
    "                    tokenized_batch[\"input_ids\"].to(device=\"cuda\"),\n",
    "                    max_length=128,\n",
    "                )\n",
    "                perturbed_texts.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "            else:\n",
    "                perturbed_texts.append(input)\n",
    "\n",
    "    perturbated_substituted_entities = []\n",
    "    for text, entity in zip(perturbed_texts, entities):\n",
    "        if entity:\n",
    "            perturbated_substituted_entities.append(replace_entity(text, entity, target_entities, \"NamedEntity\", \"GenderSwapEntity\"))\n",
    "        else:\n",
    "            perturbated_substituted_entities.append(text)\n",
    "    data['perturbed_text'] = perturbated_substituted_entities\n",
    "    data = data[[\"text\", \"perturbed_text\", \"entity\", \"labels\"]]\n",
    "\n",
    "    print(\"...texts perturbated!\")\n",
    "\n",
    "    if not os.path.exists(f\"{PROCESSED_PATH}{os.sep}{DIMENSION}\"):\n",
    "        os.mkdir(f\"{PROCESSED_PATH}{os.sep}{DIMENSION}\")\n",
    "\n",
    "    data_path = f\"{PROCESSED_PATH}{os.sep}{DIMENSION}{os.sep}{path.split('/')[-1]}\"\n",
    "    data.to_csv(data_path, index=False, header=True, encoding=\"utf-8\")\n",
    "    print(f\"{data_path} created\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "misinfo_bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
