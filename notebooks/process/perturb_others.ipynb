{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/perturber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [\"nationality\", \"country\", \"religion\"]\n",
    "entity_types = [\"NORP\", \"GPE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = \"../../data/raw\"\n",
    "PROCESSED_PATH = \"../../data/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, target_entities):\n",
    "    target_key = target_entities.keys()[0]\n",
    "\n",
    "    entity_to_substitute = None\n",
    "    # Create Doc object\n",
    "    doc2 = nlp(text)\n",
    "    # Identify the entities\n",
    "    entities = [ent.text for ent in doc2.ents if ent.label_ in entity_types]\n",
    "    if entities:\n",
    "        matched_entities = []\n",
    "        for entity in entities:\n",
    "            if any(entity.lower() in token.lower() for token in target_entities[target_key].to_list()):\n",
    "                matched_entities.append(entity)\n",
    "        if matched_entities:\n",
    "            entity_to_substitute = matched_entities[0]\n",
    "    return entity_to_substitute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_entity(text, entity, df, search_column, return_column):\n",
    "    # Check if the substring is present in the search column\n",
    "    mask = df[search_column].str.contains(entity.capitalize())\n",
    "\n",
    "    # If a match is found, return the corresponding value from the return column\n",
    "    if mask.any():\n",
    "        swap_entity = df.loc[mask, return_column].iloc[0]\n",
    "        regex = f\"([A-Z]([a-z]+|\\.)\\s*)*{entity.split(' ')[-1]}\"\n",
    "        text = re.sub(r''+regex, swap_entity, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = []\n",
    "for path in os.walk(RAW_PATH):\n",
    "    for file in path[2]:\n",
    "        if file.endswith(\"test.csv\") or file.endswith(\"train.csv\"):\n",
    "            data_paths.append(f\"{path[0]}{os.sep}{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(sents, batch_size):\n",
    "    for i in range(0, len(sents), batch_size):\n",
    "        yield sents[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dimension in dimensions:\n",
    "    target_entities = pd.read_csv(f\"../../heterogeneity/lists_for_perturbations/{dimension}_swaps.csv\")\n",
    "    for path in data_paths:\n",
    "\n",
    "        path2check = f'{PROCESSED_PATH}{os.sep}{dimension}{os.sep}{path.split(\"/\")[-1]}'\n",
    "        if os.path.exists(path2check):\n",
    "            print(f\"Already exists file: {path2check}. Skip.\")\n",
    "            continue\n",
    "\n",
    "        data = pd.read_csv(path).dropna()\n",
    "\n",
    "        print(f\"Processing {path}\")\n",
    "\n",
    "        print(\"Truncating texts...\")\n",
    "        data['text'] = data.apply(\n",
    "        lambda row: tokenizer.batch_decode(\n",
    "            tokenizer(\n",
    "                row.text,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=128,\n",
    "                truncation=True,\n",
    "            )[\"input_ids\"],\n",
    "        skip_special_tokens=True\n",
    "        )[0],\n",
    "        axis = 1\n",
    "        )\n",
    "        print(\"...texts truncated!\")\n",
    "\n",
    "        print(\"Etracting entities...\")\n",
    "        entities = []\n",
    "        for row in data[\"text\"].to_list():\n",
    "            entity = preprocess_text(row, target_entities)\n",
    "            entities.append(entity)\n",
    "        data[\"entity\"] = entities\n",
    "        print(\"...entities extracted!\")\n",
    "\n",
    "        print(\"Perturbating texts...\")\n",
    "        text_input = data[\"text\"].to_list()\n",
    "        entities = data[\"entity\"].to_list()\n",
    "        perturbed_texts = []\n",
    "        for input, entity in zip(tqdm(text_input, total=len(text_input)), entities):\n",
    "            if entity:\n",
    "                perturbed_texts.append(replace_entity(input, entity, target_entities, target_entities.keys()[0], target_entities.keys()[1]))\n",
    "            else:\n",
    "                perturbed_texts.append(input)\n",
    "        data['perturbed_text'] = perturbed_texts\n",
    "        data = data[[\"text\", \"perturbed_text\", \"entity\", \"labels\"]]\n",
    "\n",
    "        print(\"...texts perturbated!\")\n",
    "\n",
    "        if not os.path.exists(f\"{PROCESSED_PATH}{os.sep}{dimension}\"):\n",
    "            os.mkdir(f\"{PROCESSED_PATH}{os.sep}{dimension}\")\n",
    "\n",
    "        data_path = f\"{PROCESSED_PATH}{os.sep}{dimension}{os.sep}{path.split('/')[-1]}\"\n",
    "        data.to_csv(data_path, index=False, header=True, encoding=\"utf-8\")\n",
    "        print(f\"{data_path} created\")\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "misinfo_bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
