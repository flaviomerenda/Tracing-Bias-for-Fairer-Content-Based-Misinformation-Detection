{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/perturber\").to(device=\"cuda\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/perturber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION = \"race\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = \"../../data/raw\"\n",
    "PROCESSED_PATH = \"../../data/processed\"\n",
    "SEP = \"<PERT_SEP>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_entities = pd.read_csv(\"../../heterogeneity/lists_for_perturbations/race_swaps.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Race</th>\n",
       "      <th>RaceSwap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>white</td>\n",
       "      <td>hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>black</td>\n",
       "      <td>asian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Race  RaceSwap\n",
       "0  white  hispanic\n",
       "1  black     asian"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text_entity_to_substitute = (text, None)\n",
    "    entities = target_entities['Race'].to_list()\n",
    "    random.shuffle(entities)\n",
    "    for race in entities:\n",
    "        if race in text.split(\" \"):\n",
    "            substitute = target_entities.loc[target_entities['Race'] == race, 'RaceSwap'].iloc[0]\n",
    "            text_entity_to_substitute = (text, substitute)\n",
    "            return text_entity_to_substitute\n",
    "\n",
    "    return text_entity_to_substitute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = []\n",
    "for path in os.walk(RAW_PATH):\n",
    "    for file in path[2]:\n",
    "        if file.endswith(\"test.csv\") or file.endswith(\"train.csv\"):\n",
    "            data_paths.append(f\"{path[0]}{os.sep}{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(sents, batch_size):\n",
    "    for i in range(0, len(sents), batch_size):\n",
    "        yield sents[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/raw/fingerprints copy/fingerprints_train.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13200/13200 [05:13<00:00, 42.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/fingerprints_train.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/fingerprints copy/fingerprints_test.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3300/3300 [01:25<00:00, 38.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/fingerprints_test.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/clef22 copy/clef22_train.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 720/720 [00:14<00:00, 48.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/clef22_train.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/clef22 copy/clef22_test.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:04<00:00, 38.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/clef22_test.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/unifiedm2/twittercovidq2_test.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:00<00:00, 413859.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/twittercovidq2_test.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/unifiedm2/basil_train.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6367/6367 [00:14<00:00, 451.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/basil_train.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/unifiedm2/clickbait_test.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3812/3812 [00:02<00:00, 1421.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/clickbait_test.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/unifiedm2/basil_test.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1592/1592 [00:03<00:00, 527.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/basil_test.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/unifiedm2/politifact_train.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162/162 [00:15<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/politifact_train.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/unifiedm2/webis_train.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [02:06<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/webis_train.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/unifiedm2/buzzfeed_train.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136/136 [00:00<00:00, 974.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/buzzfeed_train.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/unifiedm2/twittercovidq2_train.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:00<00:00, 496.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/twittercovidq2_train.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/unifiedm2/propaganda_train.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1294/1294 [00:04<00:00, 285.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/propaganda_train.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/unifiedm2/buzzfeed_test.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 620027.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/buzzfeed_test.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/unifiedm2/propaganda_test.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 324/324 [00:01<00:00, 194.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/propaganda_test.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/unifiedm2/pheme_train.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1364/1364 [00:04<00:00, 285.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/pheme_train.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/unifiedm2/pheme_test.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 341/341 [00:01<00:00, 269.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/pheme_test.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/unifiedm2/webis_test.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [00:42<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/webis_test.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/unifiedm2/clickbait_train.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15244/15244 [00:13<00:00, 1169.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/clickbait_train.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/unifiedm2/politifact_test.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:01<00:00, 32.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/politifact_test.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/shadesoftruth copy/shadesoftruth_test.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:51<00:00, 30.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/shadesoftruth_test.csv created\n",
      "\n",
      "\n",
      "Processing ../data/raw/shadesoftruth copy/shadesoftruth_train.csv\n",
      "Truncating texts...\n",
      "...texts truncated!\n",
      "preprocessing text and extracting entities...\n",
      "...text preprocessed and entities extracted!\n",
      "Perturbating texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6400/6400 [03:18<00:00, 32.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...texts perturbated!\n",
      "../data/processed/race/shadesoftruth_train.csv created\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for path in data_paths:\n",
    "\n",
    "    path2check = f'{PROCESSED_PATH}{os.sep}{DIMENSION}{os.sep}{path.split(\"/\")[-1]}'\n",
    "    if os.path.exists(path2check):\n",
    "        print(f\"Already exists file: {path2check}. Skip.\")\n",
    "        continue\n",
    "\n",
    "    data = pd.read_csv(path).dropna()\n",
    "\n",
    "    print(f\"Processing {path}\")\n",
    "\n",
    "    print(\"Truncating texts...\")\n",
    "    data['text'] = data.apply(\n",
    "    lambda row: tokenizer.batch_decode(\n",
    "        tokenizer(\n",
    "            row.text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "        )[\"input_ids\"],\n",
    "    skip_special_tokens=True\n",
    "    )[0],\n",
    "    axis = 1\n",
    "    )\n",
    "    print(\"...texts truncated!\")\n",
    "\n",
    "    print(\"preprocessing text and extracting entities...\")\n",
    "    preprocessed_text = []\n",
    "    entities = []\n",
    "    for row in data[\"text\"].to_list():\n",
    "        text, entity = preprocess_text(row)\n",
    "        preprocessed_text.append(text)\n",
    "        entities.append(entity)\n",
    "    data[\"preprocessed_text\"] = preprocessed_text\n",
    "    data[\"entity\"] = entities\n",
    "    print(\"...text preprocessed and entities extracted!\")\n",
    "\n",
    "    data['perturber_input'] = data.apply(\n",
    "        lambda row: f\"{target_entities.loc[target_entities['RaceSwap'] == row.entity, 'Race'].iloc[0]}, {row.entity} {SEP} {row.preprocessed_text}\" \n",
    "        if row.entity else row.preprocessed_text, axis=1\n",
    "    )\n",
    "\n",
    "    print(\"Perturbating texts...\")\n",
    "    perturber_input = data[\"perturber_input\"].to_list()\n",
    "    entities = data[\"entity\"].to_list()\n",
    "    with torch.no_grad():\n",
    "        perturbed_texts = []\n",
    "        for input, entity in zip(tqdm(perturber_input, total=len(perturber_input)), entities):\n",
    "            if entity:\n",
    "                tokenized_batch = tokenizer(\n",
    "                    input,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                    max_length=128\n",
    "                )\n",
    "                outputs = model.generate(\n",
    "                    tokenized_batch[\"input_ids\"].to(device=\"cuda\"),\n",
    "                    max_length=128,\n",
    "                )\n",
    "                perturbed_texts.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "            else:\n",
    "                perturbed_texts.append(input)\n",
    "\n",
    "    data['perturbed_text'] = perturbed_texts\n",
    "    data = data[[\"text\", \"perturbed_text\", \"entity\", \"labels\"]]\n",
    "\n",
    "    print(\"...texts perturbated!\")\n",
    "\n",
    "    if not os.path.exists(f\"{PROCESSED_PATH}{os.sep}{DIMENSION}\"):\n",
    "        os.mkdir(f\"{PROCESSED_PATH}{os.sep}{DIMENSION}\")\n",
    "\n",
    "    data_path = f\"{PROCESSED_PATH}{os.sep}{DIMENSION}{os.sep}{path.split('/')[-1]}\"\n",
    "    data.to_csv(data_path, index=False, header=True, encoding=\"utf-8\")\n",
    "    print(f\"{data_path} created\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "misinfo_bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
