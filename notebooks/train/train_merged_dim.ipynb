{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from transformers import (\n",
    "    AutoAdapterModel, \n",
    "    AutoTokenizer, \n",
    "    PfeifferConfig,\n",
    "    TrainingArguments, \n",
    "    AdapterTrainer,\n",
    "    AutoConfig, \n",
    "    TrainerCallback, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    DatasetDict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../data/processed\"\n",
    "MODEL_PATH = \"../../models\"\n",
    "\n",
    "models_name = [\n",
    "    \"bert-base-cased\",\n",
    "    \"roberta-base\",\n",
    "    \"distilbert-base-cased\",\n",
    "    \"microsoft/deberta-base\",\n",
    "    \"facebook/FairBERTa\",\n",
    "]\n",
    "\n",
    "tasks = [\n",
    "    \"buzzfeed\",\n",
    "    \"politifact\",\n",
    "    \"twittercovidq2\",\n",
    "    \"clef22\",\n",
    "    \"propaganda\",\n",
    "    \"webis\",\n",
    "    \"pheme\",\n",
    "    \"basil\",\n",
    "    \"shadesoftruth\",\n",
    "    \"fingerprints\",\n",
    "    \"clickbait\",\n",
    "]\n",
    "\n",
    "dimensions = [\n",
    "    \"merged\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdapterDropTrainerCallback(TrainerCallback):\n",
    "  def on_step_begin(self, args, state, control, **kwargs):\n",
    "    skip_layers = list(range(np.random.randint(0, 11)))\n",
    "    kwargs['model'].set_active_adapters(kwargs['model'].active_adapters[0], skip_layers=skip_layers)\n",
    "\n",
    "  def on_evaluate(self, args, state, control, **kwargs):\n",
    "    # Deactivate skipping layers during evaluation (otherwise it would use the\n",
    "    # previous randomly chosen skip_layers and thus yield results not comparable\n",
    "    # across different epochs)\n",
    "    kwargs['model'].set_active_adapters(kwargs['model'].active_adapters[0], skip_layers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_name:\n",
    "    for task in tasks:\n",
    "        for dimension in dimensions:\n",
    "\n",
    "            CONFIG = {\n",
    "                \"task_name\": task,\n",
    "                \"model_name\": model_name,\n",
    "                \"max_length\": 128,\n",
    "                \"batch_size\": 32,\n",
    "                \"epochs\": 20,\n",
    "                \"seed\" : 0,\n",
    "                \"learning_rate\": 5e-4,\n",
    "            }\n",
    "\n",
    "            dataset_path = f\"{DATA_PATH}{os.sep}{dimension}\"\n",
    "\n",
    "            train_df = pd.read_csv(f\"{dataset_path}{os.sep}{CONFIG['task_name']}_train.csv\")\n",
    "            test_df = pd.read_csv(f\"{dataset_path}{os.sep}{CONFIG['task_name']}_test.csv\")\n",
    "\n",
    "            train_df = train_df.drop(columns=['text'])\n",
    "            train_df.rename(columns = {'perturbed_text':'text'}, inplace = True)\n",
    "\n",
    "            test_df = test_df.drop(columns=['text'])\n",
    "            test_df.rename(columns = {'perturbed_text':'text'}, inplace = True)\n",
    "\n",
    "            train_valid =  Dataset.from_pandas(train_df).train_test_split(0.2)\n",
    "            test = Dataset.from_pandas(test_df)\n",
    "\n",
    "            dataset = DatasetDict(\n",
    "                {\n",
    "                    \"train\": train_valid[\"train\"],\n",
    "                    \"valid\": train_valid[\"test\"],\n",
    "                    \"test\": test,\n",
    "                }\n",
    "            )\n",
    "            dataset = dataset.class_encode_column(\"labels\")\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "\n",
    "            def tokenize_function(examples):\n",
    "                return tokenizer(\n",
    "                    examples[\"text\"], padding=\"max_length\", truncation=True, max_length=CONFIG[\"max_length\"]\n",
    "                )\n",
    "\n",
    "            tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "            train_dataset = tokenized_datasets[\"train\"]\n",
    "            valid_dataset = tokenized_datasets[\"valid\"]\n",
    "            test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "            num_labels = len(set(train_dataset[\"labels\"]))\n",
    "\n",
    "            labels = train_dataset.features[\"labels\"].names\n",
    "            id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "            label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "            def get_model():\n",
    "                config = AutoConfig.from_pretrained(\n",
    "                    CONFIG[\"model_name\"],\n",
    "                    num_labels=num_labels,\n",
    "                    id2label=id2label,\n",
    "                )\n",
    "                task_model = AutoAdapterModel.from_pretrained(\n",
    "                    CONFIG[\"model_name\"],\n",
    "                    config=config\n",
    "                )\n",
    "                adapter_config = PfeifferConfig()\n",
    "                task_model.add_adapter(CONFIG[\"task_name\"], config=adapter_config)\n",
    "                task_model.train_adapter(CONFIG[\"task_name\"])\n",
    "                task_model.add_classification_head(\n",
    "                    CONFIG[\"task_name\"],\n",
    "                    num_labels=len(id2label),\n",
    "                    id2label=id2label,\n",
    "                )\n",
    "                task_model.set_active_adapters(CONFIG[\"task_name\"])\n",
    "                \n",
    "                return task_model\n",
    "\n",
    "            f1_metric = evaluate.load(\"f1\")\n",
    "            recall_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "            def compute_metrics(eval_pred):\n",
    "                logits, labels = eval_pred\n",
    "                preds = np.argmax(logits, axis=-1)\n",
    "                results = {}\n",
    "                results.update(f1_metric.compute(predictions=preds, references=labels, average=\"macro\"))\n",
    "                results.update(recall_metric.compute(predictions=preds, references=labels))\n",
    "                return results\n",
    "\n",
    "            output_dir = f\"{MODEL_PATH}{os.sep}{CONFIG['model_name']}{os.sep}{dimension}{os.sep}{task}\"\n",
    "            metric_for_best_model = \"accuracy\"\n",
    "            strategy = \"epoch\"\n",
    "            weight_decay = 0.01\n",
    "            load_best_model_at_end = True\n",
    "            save_total_limit = 1\n",
    "            early_stopping_patience = 5\n",
    "            overwrite_output_dir = True\n",
    "\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=output_dir,\n",
    "                overwrite_output_dir=overwrite_output_dir,\n",
    "                evaluation_strategy=strategy,\n",
    "                logging_strategy=strategy,\n",
    "                save_strategy=strategy,\n",
    "                learning_rate=CONFIG[\"learning_rate\"],\n",
    "                per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "                per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "                num_train_epochs=CONFIG[\"epochs\"],\n",
    "                weight_decay=weight_decay,\n",
    "                load_best_model_at_end=load_best_model_at_end,\n",
    "                metric_for_best_model=metric_for_best_model,\n",
    "                save_total_limit=save_total_limit,\n",
    "                seed=CONFIG[\"seed\"]\n",
    "            )\n",
    "\n",
    "            trainer = AdapterTrainer(\n",
    "                model_init=get_model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=valid_dataset,\n",
    "                compute_metrics=compute_metrics,\n",
    "                callbacks = [\n",
    "                    EarlyStoppingCallback(early_stopping_patience=early_stopping_patience),\n",
    "                    AdapterDropTrainerCallback()\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            trainer.train()\n",
    "            trainer.evaluate(test_dataset, metric_key_prefix=\"test\")\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "misinfo_bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
